{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import pandas as pd\n",
    "from eval_utils import rougel_eval, selfbleu_eval, bertscore_eval, bleurt_eval, concat_gen, tokenize\n",
    "from gen_answerability import run_generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TYPE2CONTROL_SIGNAL = ['What', 'How', 'Who', 'Why', 'Where', 'When', 'Which']\n",
    "\n",
    "# path for generated result\n",
    "result_dir = 'otuput/'\n",
    "result_df = pd.read_csv(result_dir)\n",
    "for i in TYPE2CONTROL_SIGNAL:\n",
    "    result_df[i] = result_df[i].apply(lambda x: ast.literal_eval(x))\n",
    "    \n",
    "# path for golden data\n",
    "test_dir = 'data/'\n",
    "gold_df = pd.read_csv(test_dir)\n",
    "gold_df = gold_df.loc[:,['cor_section', 'question']]\n",
    "gold_df.columns = ['context', 'question']\n",
    "\n",
    "# merge dataframe\n",
    "gb = gold_df.groupby(['context'])\n",
    "result = pd.DataFrame(gb['question'].unique())\n",
    "\n",
    "tot_df = pd.merge(result, result_df, on=\"context\")\n",
    "tot_df = tot_df.dropna()\n",
    "tot_df['tot_gen'] = tot_df.apply(lambda x: concat_gen(x, 4, TYPE2CONTROL_SIGNAL), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rougel_eval(tot_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selfbleu_eval(tot_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bertscore_eval(tot_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bleurt_eval(tot_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# answerability evaluation model checkpoint\n",
    "ckpt_path = 'checkpoint/epoch=5-step=1109.ckpt'\n",
    "    \n",
    "ans_df = tot_df.loc[:, ['context', 'tot_gen']]\n",
    "ans_df = ans_df.explode('tot_gen').reset_index(inplace=False, drop=True)\n",
    "\n",
    "run_generate(ans_df, ckpt_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pl1.7.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
